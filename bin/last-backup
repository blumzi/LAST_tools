#!/bin/bash

#
# Files with this name pattern will
# - not be backed up
# - will not be removed after backup
# - will be removed if more than 36 hours have passed since the transfer succeeded
#
ignored_files='/LAST.*_sci_proc_(Image|Mask|PSF)'   # A 'grep -Ev' pattern

#
# Stamps a directory as "transfered"
# Removes previous (supposedly bogus) stamps 
# Syncs the .status file to the target
#
function stamp_as_transfered() {
    local src_dir="${1}"
    local dst_dir="${2}"
    local job_tag="${3}"

    local stat_file=${src_dir}/.status
    local stamp_line="$(date --iso-8601=seconds --utc) transfered"
    local file_type
    local ssh_flags=(-q -o "StrictHostKeyChecking accept-new")

    local time_stamp=$(grep -w "ready-for-transfer" ${stat_file} | tail -1 | cut -d' ' -f1)
    stamp_line+=" # ${hours_from_ready[${src_dir}]} hours from ready for transfer"

    if [ ! -e ${stat_file} ]; then
        echo "${stamp_line}" > ${stat_file}
        message_success "${job_tag} Stamped \"${src_dir}\" as \"transfered\"."
    else
        (
            grep -vw transfered ${stat_file}
            echo "${stamp_line}"
        ) > ${stat_file}.new
        mv ${stat_file}.new ${stat_file}
    fi
    chown ${user_name}.${user_group} ${stat_file}
    message_success "${job_tag} Stamped \"${src_dir}\" as \"transfered\"."

    file_type=$( sudo -u ${user_name} ssh "${ssh_flags[@]}" ${backup_user}@${backup_host} "stat -c %F ${dst_dir#*:} 2>/dev/null" )
    if [ "${file_type}" = directory ]; then
        message_info "${job_tag} Copying .info and .status to ${dst_dir} ..."
        sudo -u ocs rsync -a ${info_dir}/ ${stat_file} ${backup_target_dir_full_path} 2>/dev/null
        #sudo -u ${user_name} scp -r "${ssh_flags[@]}" ${info_dir} ${dst_dir}
        #sudo -u ${user_name} scp "${ssh_flags[@]}" ${stat_file} ${dst_dir}
    elif [ "${file_type}" = "regular file" ]; then
        message_failure "${job_tag} Not copying .status and .info to ${dst_dir}, ${dst_dir} IS A REGULAR FILE"
    elif [ ! "${file_type}" ]; then
        message_failure "${job_tag} Not copying .status and .info to ${dst_dir}, ${dst_dir} DOES NOT EXIST"
    fi
}

function cleanup() {
    local failures=( $(if [ -s ${global_tmp}/.failures ]; then cat ${global_tmp}/.failures 2>/dev/null; fi) )
    local running_processes=( $(jobs -pr) )

    if [ ${#running_processes[*]} -eq 0 ]; then
        if (( ${#failures[*]} == 0 )); then
            # message_success "All chunks have been synchronized"
            :
        else
            failures=( $(list_sort "${failures[@]}" ) )
            message_fatal "Chunks ${failures[*]} have failed to synchronize!"
        fi
    else
        message_info "Killing running process(es): ${running_processes[*]}"
        2>/dev/null kill -9 ${running_processes[*]} 
    fi
    /bin/rm -rf ${global_tmp}
    exit 0
}

trap cleanup SIGHUP SIGINT


PROG=$(basename ${0})
source /etc/profile.d/last.sh

module_include lib/message
module_include lib/util
module_include lib/list
module_include lib/backup
module_include lib/user

util_log_invokation ${*}

# defaults
identity=
default_nproc=1
default_njobs=1
default_age_hours=36
basic_exclusion_pattern="--exclude '*/raw'"
extended_exclusion_pattern="--exclude '*_sci_proc_Image_*' --exclude '*_sci_proc_Mask_*' --exclude '*_sci_proc_PSF_*'"
rsync_extra_args="${basic_exclusion_pattern} ${extended_exclusion_pattern}"
old_age_hours=${default_age_hours}


function usage() {
    cat <<- EOF

    Usage: ${PROG} [flags] -s|--source|-f|--from <source-directory> [ -t|--to|--target [<user@host>:]<target-directory> ]

    Either one or more <source-directories> may be specified with --source.
    If no <source-directories> were specified, all the directories named */proc/* under /$(hostname -s)/data[12] will
     be considered

    Every <source-directory> will be recursivelly synchronized with the <target-directory>.
    Both source and target may reside on the current machine.

    This utility:
    - Uses an rsync 'dry-run' to get the list of out-of-date files
       between the source and the target directories (a workload list).
    - Splits the workload into chunks (maximum as many as the number of parallel processes)
    - Starts the relevant number of rsync processes, one-per-chunk
    - Waits for all the rsync processes to end.

    Exclusion pattern:
        ${rsync_extra_args}

    Flags:
     -h|--help        - Show usage and exit

     Source/destination:
     -s|--source      - Specifies a source-directory (may be repeated)
     -f|--from        - Same as -s|--source
     -t|--to|--target - Specifies the target-directory (default: ${backup_default_target})
     -F|--full        - Full backup, exclude only */raw (ignores the exclusion pattern)

    Resources:
     -j|--jobs N      - Run upto N jobs (default: ${default_njobs}) in parallel
     -p|--processes   - Specifies the number of processes to use per job (default: ${default_nproc})
     -x|--extra       - Specifies additional arguments to rsync (e.g. --exclude=...)
     -a|--age         - Specifies an aging period for visits (default: ${default_age_hours})

     -r|--remove      - Remove source directory if it has been:
                        - successfully transfered
                        - ready-for-transfer for more than ${default_age_hours} hours

    Status:
     -S|--status      - Show current status
     -R|--running     - Show running backup processes
     -v|--verbose     - Bee more talkative (can be repeated to increase verbosity)

    Cruelty:
     -k|--kill        - Kills all existing ${PROG} processes

EOF
}

getopt_err=$(mktemp)
OPTS=$( getopt -o 'aSs:f:t:hi:p:xrj:FkvcR' --long "age,running,check,kill,status,jobs,help,extra:,processes:,identity:,source:,from:,to:,target:,remove,full,verbose" -n "${PROG}" -- "$@" 2> ${getopt_err} )
if [ $? -ne 0 ]; then
    if [ -r ${getopt_err} ]; then
        message_failure "$(< ${getopt_err})"
    fi
    /bin/rm -f ${getopt_err}
    exit 1
fi
/bin/rm -f ${getopt_err}

eval set -- "${OPTS}"

#
# Shows the current backup processes, trying to figure out what spawned them
#
function show_running_processes() {
    local lines pids pid

    pids=( $( pgrep ${PROG} ) )
    if [ ${#pids[*]} -eq 0 ]; then
        message_info "No active ${PROG} processes."
        return
    fi
    module_include lib/ansi

    mapfile lines < <(pgrep -alf 'sudo.*rsync')
    if [ ${#lines[@]} -eq 0 ]; then
        message_info "No last-backup processes"
        return
    fi

    message_empty
    message_info "Current backup processes:"
    for line in "${lines[@]}"; do
        #echo "line: ${line}" 
        local -a words=( ${line} )
        local word visit from dest

        pid=${words[0]}
        local parent_process=$( pstree -aclsp ${pid} | awk -v pid=${pid} '
            {
                if (match($0, /.*last-backup.*/)) {
                    print parent_process
                    exit
                }
                sub(/.*`-/, "")
                split($1, arr, ",")
                parent_pid = arr[2]
                parent_cmd = arr[1] 
                for (i = 2; i <= NF; i++)
                    parent_cmd = parent_cmd " " $i
                parent_process = "[" parent_pid "] " parent_cmd
            }
        ')

        for word in ${words[*]}; do
            if [[ ${word} == /last* ]]; then
                visit=${word#*archive/}
                from="${word%${visit}}"
                visit="${visit%/}"
            fi


            if [[ ${word} == ${backup_user}@${backup_host}* ]]; then
                dest="${word%${visit}}"

                message_success "  Visit:       $(ansi_bold ${visit})"
                message_info    "  Source:      ${from%/}/$(ansi_bold ${visit})"
                message_info    "  Destination: ${dest%/}/$(ansi_bold ${visit})"
                message_info    "  Initiator:   ${parent_process}"
                message_info    "  Started:     $(ps -p ${pid} -o start= | tr -d ' ')"
                message_info    "  Elapsed:     $(ps -p ${pid} -o etime= | tr -d ' ')      # [[DD-]hh:]mm:ss"
                message_empty

                break
            fi
        done
    done
}

full_flag=false
remove_flag=false
candidates=()
target=
status_flag=false
help_flag=false
kill_flag=false
verbosity_level=0
check_flag=false
max_jobs=${default_njobs}
running_flag=false

while true; do
    case "${1}" in

    -a|--age)
        old_age_hours="${1}"
        shift 1
        ;;

    -R|--running)
        running_flag=true
        shift 1
        ;;

    -k|--kill)
        kill_flag=true
        shift 1
        ;;

    -S|--status)
        status_flag=true
        shift 1
        ;;

    -j|--jobs)
        max_jobs=${1}
        shift 2
        ;;

    -r|--remove)
        remove_flag=true
        shift 1
        ;;

    -v|--verbose)
        (( verbosity_level++ ))
        shift 1
        ;;

    -s|--source|-f|--from)
        source="$(realpath ${2})"
        if [ $? -ne 0 ]; then
            message_fatal "Bad source \"${2}\", no such directory"
            exit 1
        fi
        candidates+=( ${source} )
        shift 2
        ;;

    -t|--target|--to)
        if [[ ${2} != *@*:* ]]; then
            message_fatal "Bad target \"${2}\". It must have the format user-name@host-name:path-to-top-directory"
            exit 1
        fi
        target="${2}"
        shift 2
        ;;

    -h|--help)
        help_flag=true
        shift 1
        ;;

    -i|--identity)
        identity="${2}"
        shift 2
        ;;
    
    -p|--processes)
        nprocs="${2}"
        shift 2
        ;;
    
    -x|--extra)
        rsync_extra_args+=" ${2}"
        shift 2
        ;;

    -F|--full)
        full_flag=true
        shift 1
        ;;

    --)
        shift 1
        break
        ;;

    -*)
        message_error "Invalid flag '${1}'"
        usage
        exit 1
        ;;

    esac
done

if ${full_flag}; then
    rsync_extra_args="${basic_exclusion_pattern}"
fi

if ${help_flag}; then
    usage
    exit 0
fi

if [ ! "${target}" ]; then
    target="${backup_default_target}"
fi


backup_set_target "${target}"

function is_being_backed_up() {
    local searched_visit="${1}"
    local lines

    mapfile lines < <(pgrep -alf 'sudo.*rsync')
    for line in "${lines[@]}"; do
        local -a words=( ${line} )
        local word visit from to

        for word in ${words[*]}; do
            if [[ ${word} == /last* ]]; then
                visit=${word#*archive/}
                visit="${visit%/}"
                if [ "${searched_visit}" = "${visit}" ]; then
                    return 0
                fi
            fi
        done
    done
    return 1
}

function kill_all_backups() {
    local sudo_pids=( $(pgrep -f 'sudo.*rsync') )
    local sudo_pid pid descendants

    for sudo_pid in ${sudo_pids[*]}; do
        descendants=( $(pstree -a -s -p ${sudo_pid} | awk '/last-backup/ { found = 1 }; found { sub(/^[^,]*,/, "", $1); print $1}'|tac ) )
        for pid in ${descendants[*]}; do
            kill ${pid} >/dev/null 2>&1
        done
    done
}

if ${kill_flag}; then
    kill_all_backups
    exit 0
fi

if [ ! "${nprocs}" ]; then
    nprocs=${default_nproc}
fi

global_tmp=$(mktemp -d)
chmod 755 ${global_tmp}
cd ${global_tmp}
info_dir=${global_tmp}/.info-$(date +%Y-%m-%d_%H-%M-%S)
mkdir -p ${info_dir}

#
# Makes an rsync dry-run and gets the list of differing files
#  between the local and remote directories
#
# Output: list of differing files (one per line)
# Return:
#  - 0: success (files list on stdout)
#  - 1: failure (stdout not relevant)
#
function get_unsynced_files() {
    local local_dir="${1}"
    local remote_dir rc=0
    local err=$(mktemp)

    remote_dir=${backup_target}
    if is_LAST_visit ${local_dir}; then
        remote_dir+=/${local_dir#*/archive/}
    fi


    #
    # Make a verbose dry-run to get the differing files list
    # Discards:
    #  - the last 3 (status) lines
    #  - the 'sending incremental ...' line
    #  - the './' line
    #  - any of the ignored files
    #
    sudo -u ocs rsync -e 'ssh -o "StrictHostKeyChecking accept-new"' -av --dry-run ${rsync_extra_args} ${local_dir}/ ${remote_dir} 2>${err} |  \
        head -n -3 | \
        grep -Ev '^\./$' |  \
        grep -v '^sending incremental file list$' |  \
        grep -Ev "${ignored_files}"

    if [ ${PIPESTATUS[0]} -ne 0 ]; then
        if [ -s ${err} ]; then
            while read line; do
                message_error "${job_tag} ${local_dir}: rsync error:  ${line}" >&2
            done < ${err}
        fi
        rc=1
    fi

    /bin/rm -f ${err}
    return ${rc}
}


#
# Check if a data_dir is already backed_up
#
# Returns:
#  0 - no unsynced files
#  1 - there are unsynced files (on stdout)
#
function same_as_on_backup() {
    local data_dir="${1}"
    local rc ndiff
    local local_tmp=$(mktemp)

    if get_unsynced_files ${data_dir} > ${local_tmp}; then
        ndiff=$(wc -l < ${local_tmp})
        if [ ${ndiff} -eq 0 ]; then
            rc=0
        else
            rc=1
        fi

        cat ${local_tmp}
    fi
    /bin/rm -f ${local_tmp}

    return ${rc}
}

function aged_enough() {
    local directory="${1}"
    local time_stamp hours_from_ready

    status_file=${directory}/.status
    if [ ! -r ${status_file} ]; then
        message_warning "Missing ${status_file}"
        return
    fi

    time_stamp=$(grep -w "ready-for-transfer" ${status_file} | tail -1 | cut -d' ' -f1)
    hours_from_ready=$(( ( $(date +%s) - $(date --date "${time_stamp}" +%s) ) / 3600 ))

    if [ ${hours_from_ready} -ge ${old_age_hours} ]; then
        return 0
    else
        return 1
    fi
}


function is_being_backed_up() {
    pgrep -f "^rsync .*${1}.*"
}

#
# Checks if a full path is a LAST visit
#
function is_LAST_visit() {
    local path="${1}"

    if [[ ${path} == /last*/archive/* ]]; then
        return 0
    else
        return 1
    fi
}

#
# The actual backup engine, handles one backup candidate
#
function backup_one_candidate() {
    local data_dir=${1}
    local job_tag="${2}"
    local backup_target_dir_full_path visit
    local local_tmp=$(mktemp)


    if is_LAST_visit "${data_dir}"; then
        visit="${data_dir#*/archive/}"
        local backup_process=$(is_being_backed_up "${data_dir}")

        if [ "${backup_process}" ]; then
            message_info "${data_dir} is already being backed up by pid ${backup_process}"
            return
        fi
        backup_target_dir_full_path=${backup_target}/${visit}
    else
        backup_target_dir_full_path=${backup_target}
    fi

    message_empty
    message_info "${job_tag} Source: \"${data_dir}\""
    message_info "${job_tag} Target: \"${backup_target_dir_full_path}\""

    message_info "${job_tag} Gathering list of files ..."
    get_unsynced_files ${data_dir} > ${local_tmp}

    message_info "${job_tag} Splitting workload ($(wc -l < ${local_tmp}) files) among ${nproc} process(es) ..."
    local nlines=$( awk -v lines=$(wc -l < ${local_tmp}) -v nprocs=${nprocs} 'BEGIN {print int(lines/nprocs) + 1}' )
    split --lines=${nlines} < ${local_tmp}
    cp ${local_tmp} ${info_dir}/files_list
    local chunks=( x?? )
    /bin/rm -f ${local_tmp}

    sudo -u ocs ssh -o "StrictHostKeyChecking accept-new" ${backup_user}@${backup_host} mkdir -p ${backup_target_dir_full_path#*:}

    local chunk_info_prefix
    for ((i = 0; i < ${#chunks[*]}; i++)); do
        (
            message_info "${job_tag} Synchronizing chunk #$((i+1)) (of ${#chunks[*]}) ..."
            local chunk_info_prefix="${info_dir}/chunk#$((i + 1))"
            local stat_file=${chunk_info_prefix}.status

            echo "sudo -u ocs rsync -aRz --stats --times ${rsync_extra_args} --files-from=${chunks[${i}]} ${data_dir}/ ${backup_target_dir_full_path} 2>/dev/null ) > ${stat_file}" > "${chunk_info_prefix}.cmd"
            cp ${chunks[${i}]} "${chunk_info_prefix}.files"
            (sudo -u ocs rsync -aRz --stats --times ${rsync_extra_args} --files-from=${chunks[${i}]} ${data_dir}/ ${backup_target_dir_full_path} 2>/dev/null ) > ${stat_file}
            local status=${?}
            echo "exit status: ${status}" >> ${stat_file}

            if [ "${status}" ] && [ ${status} -eq 0 ]; then
                Speedup=$(grep speedup ${stat_file}  | cut -d' ' -f8)
                    Bps=$(grep '^sent ' ${stat_file} | cut -d' ' -f9 | tr -d ,)
                   Mbps=$(awk -v Bps=${Bps} 'BEGIN { printf "%.3f\n", Bps / 125000 }')
                message_success "${job_tag} Chunk #$((i+1)) (of ${#chunks[*]}) succeeded (${Mbps} Mbps, speedup ${Speedup})"
            else
                # rsync returns 20 when killed
                if [ "${status}" != 20 ]; then
                    message_failure "${job_tag} Chunk #$((i+1)) (of ${#chunks[*]}) failed with status: ${status}"
                fi
                echo ${i} >> ${global_tmp}/.failures
            fi
        ) &
    done

    wait

    if [ ! -e ${global_tmp}/.failures ]; then
        # the backup succeeded
        stamp_as_transfered "${data_dir}" "${backup_target_dir_full_path}" "${job_tag}"
    fi
}

#
# Main
#
if [ "${#candidates[@]}" -eq 0 ]; then     # no candidates were selected with --source
    if (( verbosity_level > 0 )); then message_info "Detecting candidates ..."; fi

    candidates=( $( find /$(hostname -s)/data[12]/archive -type d -path '*/proc/*' | grep -v '_re/' | sort ) )
fi

candidates_for_backup=()
candidates_for_removal=()
missing_ready_stamp=()
missing_status_file=()
still_young=()
candidates_for_retry=()
declare -A hours_from_ready=()

#
# The preamble.
# Gather information about the candidates (either specified by the user or found on the filesystems)
#
for candidate in ${candidates[*]}; do
    stat_file=${candidate}/.status
    time_stamp=
    hours=

    if [ ! -r ${stat_file} ]; then
        missing_status_file+=( ${candidate} )
        continue
    fi

    time_stamp=$(grep -w "ready-for-transfer" ${stat_file} | tail -1 | cut -d' ' -f1)
    if [ ! "${time_stamp}" ]; then
        missing_ready_stamp+=( ${candidate} )
        continue
    fi

    if grep -qw transfered ${stat_file}; then

        hours=$(( ( $(date +%s) - $(date --date "${time_stamp}" +%s) ) / 3600 ))

        hours_from_ready[${candidate}]=${hours}
        if (( hours >= old_age_hours )); then
            candidates_for_removal+=( ${candidate} )
        else
            still_young+=( ${candidate} )
        fi

    else
        candidates_for_backup+=( ${candidate} )
    fi

done

#
# The status part
#
if ${status_flag}; then
    message_info "Found ${#candidates[*]} candidates"

    message_empty
    message_info "Found $(printf "%4d" ${#missing_status_file[*]}) candidates with no .status file"
    if (( verbosity_level > 0 )); then
        for candidate in ${missing_status_file[*]}; do
            message_info "  ${candidate}"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#missing_ready_stamp[*]}) candidates with no ready-for-transfer stamp"
    if (( verbosity_level > 0 )); then
        for candidate in ${missing_ready_stamp[*]}; do
            message_info "  ${candidate}"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#candidates_for_backup[*]}) candidates for backup"
    if (( verbosity_level > 0 )); then
        for candidate in ${candidates_for_backup[*]}; do
            message_info "  ${candidate}"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#candidates_for_removal[*]}) candidates old enough for removal (ready-for-transfer for more than ${old_age_hours} hours)"
    if (( verbosity_level > 0 )); then
        for candidate in ${candidates_for_removal[*]}; do
            message_info "  ${candidate}: $(printf "%4d" ${hours_from_ready[${candidate}]}) hours"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#still_young[*]}) candidates too young for removal (ready-for-transfer for less than ${old_age_hours} hours)"
    if (( verbosity_level > 0 )); then
        for candidate in ${still_young[*]}; do
            message_info "  ${candidate}: $(printf "%4d" ${hours_from_ready[${candidate}]}) hours"
        done
    fi

    if ${running_flag}; then
        show_running_processes
    fi

    exit 0
fi

if ${running_flag} && ! ${status_flag}; then
    show_running_processes
    exit 0
fi


#
# The removal part
# Remove local directories only if they are the same as the backup
#
if ${remove_flag} && [ ${#candidates_for_removal[*]} -ne 0 ]; then
    message_info "Considering ${#candidates_for_removal[*]} candidates for removal ..."

    for candidate in ${candidates_for_removal[*]}; do
        diffs=$(mktemp)
        if same_as_on_backup ${candidate} >${diffs}; then
            /bin/rm -rf ${candidate}
            message_success "  ${candidate} - removed"
        else
            message_error "  ${candidate} - differs from backup ($(wc -l < ${diffs}) diffs), will retry"
            candidates_for_retry+=( ${candidate} )  # give it another chance
        fi
        /bin/rm ${diffs}
    done
fi

#
# The backup part
# Backup candidates that are ready-for-transfer but:
#  - are not marked-as-transfered 
#  - are marked-as-transfered but not really backed up (either backup error or different from the backup)
#

total_candidates_for_backup=( ${candidates_for_backup[*]} ${candidates_for_retry[*]} )
if [ ${#total_candidates_for_backup[*]} -ne 0 ]; then
    if (( verbosity_level > 0 )); then
        message_info "Considering ${#total_candidates_for_backup[*]} candidates for backup (or retry)"
    fi

    running_jobs=0
    job_id=0
    for candidate in ${total_candidates_for_backup[*]}; do
        (( job_id++ ))

        backup_process=$(is_being_backed_up ${candidate})
        if [ "${backup_process}" ]; then
            message_success "${candidate}: is currently being backed up by process ${backup_process}, skipped"
            continue
        fi

        backup_one_candidate ${candidate} "[${job_id}/${#total_candidates_for_backup[*]}]" &
        (( running_jobs++ ))

        if ((running_jobs >= max_jobs)); then
            wait -n
            ((running_jobs--))
        fi
    done
else
    if (( verbosity_level > 0 )); then
        message_ok "No candidates for backup"
    fi
fi

cleanup
