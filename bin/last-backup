#!/bin/bash

#
# Files with this name pattern will
# - not be backed up
# - will not be removed after backup
# - will be removed if more than 36 hours have passed since the transfer succeeded
#
special_files='LAST.*_sci_proc_proc_(Image|Mask)'

#
# Stamps a directory as "transfered"
# Removes previous (supposedly bogus) stamps 
# Syncs the .status file to the target
#
function stamp_as_transfered() {
    local dir="${1}"
    local stat_file=${dir}/.status
    local date=$(date --iso-8601=seconds)   # 2024-03-11T08:23+00:00
    local stamp_line="${date%+*} transfered"

    if [ ! -e ${stat_file} ]; then
        echo "${stamp_line}" > ${stat_file}
        message_success "Stamped \"${source}\" as \"transfered\"."
	else
        (
            grep -vw transfered ${stat_file}
            echo "${stamp_line}"
        ) > ${stat_file}.new
        mv ${stat_file}.new ${stat_file}
    fi
    chown ${user_name}.${user_group} ${stat_file}
    message_success "Stamped \"${source}\" as \"transfered\"."
    sudo -u ${user_name} scp -q -o "StrictHostKeyChecking accept-new" ${stat_file} ${backup_user}@${backup_host}:${backup_topdir}/${data_dir}
    sudo -u ${user_name} scp -r -q -o "StrictHostKeyChecking accept-new" ${info_dir} ${backup_user}@${backup_host}:${backup_topdir}/${data_dir}
}

function cleanup() {
    local failures=( $(if [ -s ${tmp}/.failures ]; then cat ${tmp}/.failures 2>/dev/null; fi) )
    local running_processes=( $(jobs -pr) )

    if [ ${#running_processes[*]} -eq 0 ]; then
        if (( ${#failures[*]} == 0 )); then
            # message_success "All chunks have been synchronized"
            :
        else
            failures=( $(list_sort "${failures[@]}" ) )
            message_fatal "Chunks ${failures[*]} have failed to synchronize!"
        fi
    else
        message_info "Killing running process(es): ${running_processes[*]}"
        2>/dev/null kill -9 ${running_processes[*]} 
    fi
    /bin/rm -rf ${tmp}
    exit 0
}

trap cleanup SIGHUP SIGINT


PROG=$(basename ${0})
source /etc/profile.d/last.sh

module_include lib/message
module_include lib/util
module_include lib/list
module_include lib/backup
module_include lib/user

util_log_invokation ${*}

# defaults
identity=
default_nproc=1
rsync_extra_args="--exclude '*/raw'"
rsync_info_args="--info=STATS0 --info=FLIST0"
quiet=false


function usage() {
    cat <<- EOF

    Usage: ${PROG} [flags] -s|--source|-f|--from <source-directory> [ -t|--to|--target [<user@host>:]<target-directory> ]

    Either one or more <source-directories> may be specified with --source.
    If no <source-directories> were specified, all the directories named */proc/* under /$(hostname -s)/data[12] will
     be backed up.

    Every <source-directory> will be recursivelly synchronized with the <target-directory>.
    Both source and target may reside on the current machine.

    This utility:
    - Uses an rsync 'dry-run' to get the list of out-of-date files
       between the source and the target directories (a workload list).
    - Splits the workload into chunks (maximum as many as the number of parallel processes)
    - Starts the relevant number of rsync processes, one-per-chunk
    - Waits for all the rsync processes to end.

    Flags:
     -h|--help        - Show usage and exit
     -a|--all         - Backup all /$(hostname -s)/data[12]/archive/*/proc/* directories (default)
     -q|--quiet       - Be quiet
     -s|--source      - Specifies the source-directory
     -f|--from        - Specifies the source-directory
     -t|--to|--target - Specifies the target-directory (default: ${backup_default_target})
     -p|--processes   - Specifies the number of processes to use (default: ${default_nproc})
     -x|--extra       - Specifies additional arguments to rsync (e.g. --exclude=...)
     -c|--check       - Check if the source directory was backed up
     -r|--remove      - Remove source directory if it was backed up (only works with -c|--check or -C|--check-all)
     -F|--force       - Force removal of local directory (only works with -r|--remove)
     -C|--check-all   - Same as -c|--check but for all the local directories
     -j|--jobs N      - Run upto N jobs (default: 5) in parallel
     -S|--status      - Show all the rsync process currently in progress

EOF
}

OPTS=$( getopt -o 'Sqs:f:t:hi:p:xcrCFaj:' --long "status,jobs,all,quiet:,help,extra:,processes:,identity:,force,source:,from:,to:,target:,check,remove,check-all" -n "${PROG}" -- "$@" )
eval set -- "${OPTS}"

check=false
check_all=false
backup_all=true
remove=false
force=false
sources=()
target=
status_only=false
jobs=1

while true; do
    case "${1}" in

    -S|--status)
        status_only=true
        shift 1
        ;;

    -j|--jobs)
        jobs=${1}
        shift 2
        ;;

    -a|--all)
        backup_all=true
        shift 1
        ;;

    -C|--check-all)
        check_all=true
        shift 1
        ;;

    -c|--check)
        check=true
        shift 1
        ;;

    -r|--remove)
        remove=true
        shift 1
        ;;

    -F|--force)
        force=true
        shift 1
        ;;

    -q|--quiet)
        quiet=true
        shift 1
        ;;

    -s|--source|-f|--from)
        source="$(realpath ${2})"
        if [ $? -ne 0 ]; then
            message_fatal "Bad source \"${2}\", no such directory"
            exit 1
        fi
        sources+=( ${source} )
        shift 2
        ;;

    -t|--target|--to)
        if [[ ${2} != *@*:* ]]; then
            message_fatal "Bad target \"${2}\". It must have the format user-name@host-name:path-to-top-directory"
            exit 1
        fi
        target="${2}"
        shift 2
        ;;

    -h|--help)
        usage
        exit 0
        ;;

    -i|--identity)
        identity="${2}"
        shift 2
        ;;
    
    -p|--processes)
        nprocs="${2}"
        shift 2
        ;;
    
    -x|--extra)
        rsync_extra_args="${2}"
        shift 2
        ;;

	--)
		shift 1
		break
		;;
	esac
done

if [ ! "${target}" ]; then
    target="${backup_default_target}"
fi

if ${quiet}; then
    export LAST_TOOL_QUIET=true
fi


backup_set_target "${target}"

function status() {
    local lines pids

    pids=( $( pgrep ${PROG} ) )
    if [ ${#pids[*]} -eq 0 ]; then
        message_info "No active ${PROG} processes."
        return
    fi
    module_include lib/ansi

    mapfile lines < <(pgrep -alf 'sudo.*rsync')

    for line in "${lines[@]}"; do
        local -a words=( ${line} ) word visit from to

        for word in ${words[*]}; do
            if [[ ${word} == /last* ]]; then
                visit=${word#*archive/}
                from="${word%${visit}}"
                visit="${visit%/}"
            fi

            if [[ ${word} == ${backup_user}@${backup_host}* ]]; then
                to="${word%${visit}}"

                message_success "Visit $(ansi_bright_green ${visit}) is being backed up from $(ansi_underline ${from%/}) to $(ansi_underline ${to%/})"
                break
            fi
        done
    done
}

if ${status_only}; then
    status
    exit 0
fi

if [ ! "${nprocs}" ]; then
    nprocs=${default_nproc}
fi

tmp=$(mktemp -d)
chmod 755 ${tmp}
cd ${tmp}
info_dir=${tmp}/.info-$(date +%Y-%m-%d@%H:%M:%S)
mkdir -p ${info_dir}

files_list=files

if ${check}; then
    backup_check_one "${source}"
    exit 0
fi

if ${check_all}; then
    backup_check_all
    exit 0
fi

pids=( $(pgrep ${PROG}) )
if [ ${#pids[*]} -ne 0 ]; then
    message_failure "There already is/are running ${PROG} process(es) (pid(s): ${pids[*]}), only one allowed!"
    exit 1
fi

if [ "${#sources[@]}" -eq 0 ]; then     # no sources were selected with --source
	if ${backup_all}; then
	    sources=( $( find /$(hostname -s)/data[12]/archive -type d -path '*/proc/*' | sort ) )
	fi
fi

# Neither manually specified nor automatically found sources
if [ "${#sources[@]}" -eq 0 ]; then
    message_info "Nothing to backup, bye!"
    exit 0
fi


function remove_if_transfered_and_aged() {
    local directory="${1}"
    local tag="${2}"
    local time_stamp hours_from_creation status_file transfered=false
    local aging_hours=36

    status_file=${directory}/.status

    if grep -wq transfered ${status_file}; then
        transfered=true
    fi
    time_stamp=$(grep -w "ready-for-transfer" ${status_file} | tail -1 | cut -d' ' -f1)
    hours_from_creation=$(( ( $(date +%s) - $(date --date "${time_stamp}" +%s) ) / 3600 ))

    if ${transfered} && [ ${hours_from_creation} -ge ${aging_hours} ]; then
        message_success "${tag} ${hours_from_creation} have passed since transfer (more than ${aging_hours}), removing \"${directory}\" ..."
        /bin/rm -rf ${directory}
    fi
}

function backup_one_source() {
    local data_dir=${1}
    local job_tag="[${job_id}/${#sources[@]}]"

    if [[ "${data_dir}"  == */archive/* ]]; then
        data_dir="${data_dir#*/archive/}"
    fi

    echo ''
    message_info "${job_tag} Source: \"${source}\""
    message_info "${job_tag} Target: \"${backup_target}/${data_dir}\""

    message_info "${job_tag} Gathering list of files ..."

    sudo -u ocs rsync -e 'ssh -o "StrictHostKeyChecking accept-new"' -av ${rsync_info_args} --dry-run ${rsync_extra_args} ${source}/ ${backup_target}/${data_dir} | grep -v '\.\/' | grep -v 'created directory' | grep -v "${special_files}" > ${files_list}
    if [ ${PIPESTATUS[0]} -ne 0 ]; then
        message_failure "${job_tag} Failed getting files list from ${backup_target}/${data_dir}"
        return
    fi

    if (( $(wc -l < ${files_list}) == 0 )); then
        message_success "${job_tag} The source and target directories are synchronized"
        stamp_as_transfered ${source}

        if ${remove} && ${force}; then
            message_success "${job_tag} Removing \"${source}\" ..."
            remove_if_transfered_and_aged ${source} "${job_tag}"
        fi

        return
    fi

    message_info "${job_tag} Splitting workload among ${nproc} processes ..."
    nlines=$( awk -v lines=$(wc -l < ${files_list}) -v nprocs=${nprocs} 'BEGIN {print int(lines/nprocs) + 1}' )
    split --lines=${nlines} < ${files_list}
    cp ${files_list} ${info_dir}/${files_list}
    chunks=( x?? )

    sudo -u ocs ssh -o "StrictHostKeyChecking accept-new" ${backup_user}@${backup_host} mkdir -p ${backup_topdir}/${data_dir}

    for ((i = 0; i < ${#chunks[*]}; i++)); do
        (
            message_info "${job_tag} Synchronizing chunk #$((i+1)) (of ${#chunks[*]}) ..."
            stat_file=stat.${i}
            chunk_info="${info_dir}/chunk#$((i + 1))"
            echo "sudo -u ocs rsync -aRz --stats --times ${rsync_extra_args} --files-from=${chunks[${i}]} ${source}/ ${backup_target}/${data_dir} 2>/dev/null ) > ${stat_file}" > "${chunk_info}.cmd"
            cp ${chunks[${i}]} "${chunk_info}.files"
            (sudo -u ocs rsync -aRz --stats --times ${rsync_extra_args} --files-from=${chunks[${i}]} ${source}/ ${backup_target}/${data_dir} 2>/dev/null ) > ${stat_file}
            status=${?}
            echo ${status} > ${chunk_info}.status

            if [ "${status}" ] && [ ${status} -eq 0 ]; then
                Speedup=$(grep speedup ${stat_file}  | cut -d' ' -f8)
                    Bps=$(grep '^sent ' ${stat_file} | cut -d' ' -f9 | tr -d ,)
                   Mbps=$(awk -v Bps=${Bps} 'BEGIN { printf "%.3f\n", Bps / 125000 }')
                message_success "${job_tag} Chunk #$((i+1)) (of ${#chunks[*]}) succeeded (${Mbps} Mbps, speedup ${Speedup})"
            else
                # rsync returns 20 when killed
                if [ "${status}" != 20 ]; then
                    message_failure "${job_tag} Chunk #$((i+1)) (of ${#chunks[*]}) failed with status: ${status}"
                fi
                echo ${i} >> ${tmp}/.failures
            fi
        ) &
    done

    wait

    if [[ ${source} == */proc/* ]] && [ ! -e ${tmp}/.failures ]; then
        # the backup succeeded
        stamp_as_transfered ${source}

        if ${remove} && ${force}; then
            message_success "${job_tag} Removing \"${source}\" ..."
            remove_if_transfered_and_aged ${source} "${job_tag}"
        fi
    fi
}

running_jobs=0
job_id=0
for source in "${sources[@]}"; do
    backup_one_source "${source}" &
    (( running_jobs++ ))
    (( job_id++ ))

    if ((running_jobs >= jobs)); then
        wait -n
        ((running_jobs--))
    fi
done

cleanup
