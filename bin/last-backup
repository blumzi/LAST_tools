#!/bin/bash

#
# Files with this name pattern will
# - not be backed up
# - will not be removed after backup
# - will be removed if more than 36 hours have passed since the transfer succeeded
#
ignored_files='/LAST.*_sci_proc_(Image|Mask|PSF)'   # A 'grep -Ev' pattern

#
# Stamps a directory as "transfered"
# Removes previous (supposedly bogus) stamps 
# Syncs the .status file to the target
#
function stamp_as_transfered() {
    local src_dir="${1}"
    local dst_dir="${2}"
    local job_tag="${3}"

    local stat_file=${src_dir}/.status
    local stamp_line="$(date --iso-8601=seconds --utc) transfered"
    local file_type
    local ssh_flags=(-q -o "StrictHostKeyChecking accept-new")

    local time_stamp=$(grep -w "ready-for-transfer" ${stat_file} | tail -1 | cut -d' ' -f1)
    stamp_line+=" # ${hours_from_ready[${src_dir}]} hours from ready for transfer"

    if [ ! -e ${stat_file} ]; then
        echo "${stamp_line}" > ${stat_file}
        message_success "${job_tag} Stamped \"${src_dir}\" as \"transfered\"."
    else
        (
            grep -vw transfered ${stat_file}
            echo "${stamp_line}"
        ) > ${stat_file}.new
        mv ${stat_file}.new ${stat_file}
    fi
    chown ${user_name}.${user_group} ${stat_file}
    message_success "${job_tag} Stamped \"${src_dir}\" as \"transfered\"."

    file_type=$( sudo -u ${user_name} ssh "${ssh_flags[@]}" ${backup_user}@${backup_host} "stat -c %F ${dst_dir#*:} 2>/dev/null" )
    if [ "${file_type}" = directory ]; then
        message_info "${job_tag} Copying .status to ${dst_dir} ..."
        sudo -u ocs rsync -a ${stat_file} ${backup_target_dir_full_path} 2>/dev/null
        if [ -d .info ]; then
            message_info "${job_tag} Copying .info to ${dst_dir} ..."
            sudo -u ocs rsync -a .info/ ${backup_target_dir_full_path} 2>/dev/null
        fi
    elif [ "${file_type}" = "regular file" ]; then
        message_failure "${job_tag} Not copying .status and .info to ${dst_dir}, ${dst_dir} IS A REGULAR FILE"
    elif [ ! "${file_type}" ]; then
        message_failure "${job_tag} Not copying .status and .info to ${dst_dir}, ${dst_dir} DOES NOT EXIST"
    fi
}

function cleanup() {
    local running_processes=( $(jobs -pr) )

    if [ ${#running_processes[*]} -gt 0 ]; then
        message_info "Killing running process(es): ${running_processes[*]}"
        2>/dev/null kill -9 ${running_processes[*]} 
    fi
    exit 0
}

trap cleanup SIGHUP SIGINT SIGTERM


PROG=$(basename ${0})
source /etc/profile.d/last.sh

module_include lib/message
module_include lib/util
module_include lib/list
module_include lib/backup
module_include lib/user

util_log_invokation ${*}

# defaults
identity=
default_nprocs=3
default_njobs=1
basic_exclusion_pattern="--exclude '*/raw'"
extended_exclusion_pattern="--exclude '*_sci_proc_Image_*' --exclude '*_sci_proc_Mask_*' --exclude '*_sci_proc_PSF_*'"
rsync_extra_args="${basic_exclusion_pattern} ${extended_exclusion_pattern}"

default_old_age_hours=36
old_age_hours=${default_old_age_hours}


function usage() {
    cat <<- EOF

    Usage: ${PROG} [flags] -s|--source|-f|--from <source-directory> [ -t|--to|--target [<user@host>:]<target-directory> ]

    Either one or more <source-directories> may be specified with --source.
    If no <source-directories> were specified, all the directories named */proc/* under /$(hostname -s)/data[12] will
     be considered

    Every <source-directory> will be recursivelly synchronized with the <target-directory>.
    Both source and target may reside on the current machine.

    This utility:
    - Uses an rsync 'dry-run' to get the list of out-of-date files
       between the source and the target directories (a workload list).
    - Splits the workload into chunks (maximum as many as the number of parallel processes)
    - Starts the relevant number of rsync processes, one-per-chunk
    - Waits for all the rsync processes to end.

    Exclusion pattern:
        ${rsync_extra_args}

    Flags:
     -h|--help        - Show usage and exit

     Source/destination:
     -s|--source      - Specifies a source-directory (may be repeated)
     -f|--from        - Same as -s|--source
     -t|--to|--target - Specifies the target-directory (default: ${backup_default_target})
     -F|--full        - Full backup, exclude only */raw (ignores the exclusion pattern)

    Resources:
     -j|--jobs N      - Run upto N jobs (default: ${default_njobs}) in parallel
     -p|--processes   - Specifies the number of processes to use per job (default: ${default_nprocs})
     -x|--extra       - Specifies additional arguments to rsync (e.g. --exclude=...)

     -a|--age         - Specify old age (in hours) (default: ${default_old_age_hours})
     -r|--remove      - Remove source directory if it has been:
                        - successfully transfered
                        - ready-for-transfer for more than ${old_age_hours} hours

    Status:
     -S|--status      - Show current status
     -R|--running     - Show running backup processes
     -v|--verbose     - Bee more talkative (can be repeated to increase verbosity)

    Cruelty:
     -k|--kill        - Kills all existing ${PROG} processes

EOF
}

getopt_err=$(mktemp)
OPTS=$( getopt -o 'Ss:f:t:hi:p:xrj:FkvcRa' --long "age:,running,check,kill,status,jobs,help,extra:,processes:,identity:,source:,from:,to:,target:,remove,full,verbose" -n "${PROG}" -- "$@" 2> ${getopt_err} )
if [ $? -ne 0 ]; then
    if [ -r ${getopt_err} ]; then
        message_failure "$(< ${getopt_err})"
    fi
    /bin/rm -f ${getopt_err}
    exit 1
fi
/bin/rm -f ${getopt_err}

eval set -- "${OPTS}"

#
# Shows the current backup processes, trying to figure out what spawned them
#
function show_running_processes() {
    local lines pids pid

    pids=( $( pgrep ${PROG} ) )
    if [ ${#pids[*]} -eq 0 ]; then
        message_info "No active ${PROG} processes."
        return
    fi
    module_include lib/ansi

    mapfile lines < <(pgrep -alf 'sudo.*rsync')
    if [ ${#lines[@]} -eq 0 ]; then
        message_info "No last-backup processes"
        return
    fi

    message_empty
    message_info "Current backup processes:"
    for line in "${lines[@]}"; do
        #echo "line: ${line}" 
        local -a words=( ${line} )
        local word visit from dest

        pid=${words[0]}
        local parent_process=$( pstree -aclsp ${pid} | awk -v pid=${pid} '
            {
                if (match($0, /.*last-backup.*/)) {
                    print parent_process
                    exit
                }
                sub(/.*`-/, "")
                split($1, arr, ",")
                parent_pid = arr[2]
                parent_cmd = arr[1] 
                for (i = 2; i <= NF; i++)
                    parent_cmd = parent_cmd " " $i
                parent_process = "[" parent_pid "] " parent_cmd
            }
        ')

        for word in ${words[*]}; do
            if [[ ${word} == /last* ]]; then
                visit=${word#*archive/}
                from="${word%${visit}}"
                visit="${visit%/}"
            fi


            if [[ ${word} == ${backup_user}@${backup_host}* ]]; then
                dest="${word%${visit}}"

                message_success "  Visit:       $(ansi_bold ${visit})"
                message_info    "  Source:      ${from%/}/$(ansi_bold ${visit})"
                message_info    "  Destination: ${dest%/}/$(ansi_bold ${visit})"
                message_info    "  Initiator:   ${parent_process}"
                message_info    "  Started:     $(ps -p ${pid} -o start= | tr -d ' ')"
                message_info    "  Elapsed:     $(ps -p ${pid} -o etime= | tr -d ' ')      # [[DD-]hh:]mm:ss"
                message_empty

                break
            fi
        done
    done
}

full_flag=false
remove_flag=false
candidates=()
target=
status_flag=false
help_flag=false
kill_flag=false
verbosity_level=0
check_flag=false
max_jobs=${default_njobs}
running_flag=false

while true; do
    case "${1}" in

    -a|--age)
        old_age_hours=${2}
        shift 2
        ;;

    -R|--running)
        running_flag=true
        shift 1
        ;;

    -k|--kill)
        kill_flag=true
        shift 1
        ;;

    -S|--status)
        status_flag=true
        shift 1
        ;;

    -j|--jobs)
        max_jobs=${1}
        shift 2
        ;;

    -r|--remove)
        remove_flag=true
        shift 1
        ;;

    -v|--verbose)
        (( verbosity_level++ ))
        shift 1
        ;;

    -s|--source|-f|--from)
        source="$(realpath ${2})"
        if [ $? -ne 0 ]; then
            message_fatal "Bad source \"${2}\", no such directory"
            exit 1
        fi
        candidates+=( ${source} )
        shift 2
        ;;

    -t|--target|--to)
        if [[ ${2} != *@*:* ]]; then
            message_fatal "Bad target \"${2}\". It must have the format user-name@host-name:path-to-top-directory"
            exit 1
        fi
        target="${2}"
        shift 2
        ;;

    -h|--help)
        help_flag=true
        shift 1
        ;;

    -i|--identity)
        identity="${2}"
        shift 2
        ;;
    
    -p|--processes)
        nprocs="${2}"
        shift 2
        ;;
    
    -x|--extra)
        rsync_extra_args+=" ${2}"
        shift 2
        ;;

    -F|--full)
        full_flag=true
        shift 1
        ;;

    --)
        shift 1
        break
        ;;

    -*)
        message_error "Invalid flag '${1}'"
        usage
        exit 1
        ;;

    esac
done

if ${full_flag}; then
    rsync_extra_args="${basic_exclusion_pattern}"
fi

if ${help_flag}; then
    usage
    exit 0
fi

if [ ! "${target}" ]; then
    target="${backup_default_target}"
fi


backup_set_target "${target}"

function is_being_backed_up() {
    local searched_visit="${1}"
    local lines

    mapfile lines < <(pgrep -alf 'sudo.*rsync')
    for line in "${lines[@]}"; do
        local -a words=( ${line} )
        local word visit from to

        for word in ${words[*]}; do
            if [[ ${word} == /last* ]]; then
                visit=${word#*archive/}
                visit="${visit%/}"
                if [ "${searched_visit}" = "${visit}" ]; then
                    return 0
                fi
            fi
        done
    done
    return 1
}

function kill_all_backups() {
    local sudo_pids=( $(pgrep -f 'sudo.*rsync') )
    local sudo_pid pid descendants

    for sudo_pid in ${sudo_pids[*]}; do
        descendants=( $(pstree -a -s -p ${sudo_pid} | awk '/last-backup/ { found = 1 }; found { sub(/^[^,]*,/, "", $1); print $1}'|tac ) )
        for pid in ${descendants[*]}; do
            kill ${pid} >/dev/null 2>&1
        done
    done
}

if ${kill_flag}; then
    kill_all_backups
    exit 0
fi

if [ ! "${nprocs}" ]; then
    nprocs=${default_nprocs}
fi

#
# Makes an rsync dry-run and gets the list of differing files
#  between the local and remote directories
#
# Output: list of differing files (one per line)
# Return:
#  - 0: success (files list on stdout)
#  - 1: failure (stdout not relevant)
#
function get_unsynced_files() {
    local local_dir="${1}"
    local remote_dir rc=0
    local err=$(mktemp)

    remote_dir=${backup_target}
    if is_LAST_visit ${local_dir}; then
        remote_dir+=/${local_dir#*/archive/}
    fi


    #
    # Make a verbose dry-run to get the differing files list
    # Discards:
    #  - the last 3 (status) lines
    #  - the 'sending incremental ...' line
    #  - the './' line
    #  - any of the ignored files
    #
    sudo -u ocs rsync -e 'ssh -o "StrictHostKeyChecking accept-new"' -av --dry-run ${rsync_extra_args} ${local_dir}/ ${remote_dir} 2>${err} |  \
        head -n -3 | \
        grep -Ev '^\./$' |  \
        grep -v '^sending incremental file list$' |  \
        grep -Ev "${ignored_files}"

    if [ ${PIPESTATUS[0]} -ne 0 ]; then
        if [ -s ${err} ]; then
            while read line; do
                message_error "${job_tag} ${local_dir}: rsync error:  ${line}" >&2
            done < ${err}
        fi
        rc=1
    fi

    /bin/rm -f ${err}
    return ${rc}
}


#
# Check if a data_dir is already backed_up
#
# Returns:
#  0 - no unsynced files
#  1 - there are unsynced files (on stdout)
#
function was_successfully_tranfered() {
    local data_dir="${1}"
    local rc ndiffs
    local diffs=$(mktemp)

    if get_unsynced_files ${data_dir} > ${diffs}; then
        ndiffs=$(wc -l < ${diffs})
        local remote_status=$(mktemp)

        case ${ndiffs} in
            0)
                rc=0
                ;;
            1)
                if [ "$(< ${diffs})" = ".status" ] && grep -qw transfered ${data_dir}/.status; then
                    rc=0
                else
                    rc=1
                fi

#                if [ "$(< ${diffs})" = ".status" ]; then
#                    local remote_dir=${backup_target}
#
#                    if is_LAST_visit ${data_dir}; then
#                        remote_dir+=/${local_dir#*/archive/}
#                    fi
#
#                    #
#                    # if the remote .status file starts with the contents of the local .status file, they are considered the same
#                    #
#                    local nbytes=$(wc -c < ${data_dir}/.status)
#
#                    ssh ${backup_user}@${backup_host} cat ${remote_dir#*:}/.status > ${remote_status}
#                    (
#                        echo === local .status =================================
#                        cat ${data_dir}/.status
#                        echo === remote .status =================================
#                        cat ${remote_status}
#                        echo ======================================================================
#                    ) >&2 
#                    if cmp --silent <(head -c ${nbytes} ${remote_status}) ${data_dir}/.status; then
#                        echo === same .status === >&2
#                        rc=0
#                    else
#                        echo === not same .status ============================= >&2 
#                        rc=1
#                    fi
#                else
#                    (
#                        echo === one diff, not .status ============================================
#                        cat ${diffs}
#                        echo ======================================================================
#                    ) >&2 
#                    rc=1
#                fi
                ;;
            *)
                rc=1
                ;;
        esac
        /bin/rm -f ${remote_status}

        cat ${diffs}
    fi
    /bin/rm -f ${diffs}

    return ${rc}
}


function is_being_backed_up() {
    pgrep -f "^rsync .*${1}.*"
}

#
# Checks if a full path is a LAST visit
#
function is_LAST_visit() {
    local path="${1}"

    if [[ ${path} == /last*/archive/* ]]; then
        return 0
    else
        return 1
    fi
}

#
# The actual backup engine, handles one backup candidate
#
function backup_one_candidate() {
    local data_dir=${1}
    local job_tag="${2}"
    local backup_target_dir_full_path visit
    local diffs=$(mktemp)


    if is_LAST_visit "${data_dir}"; then
        visit="${data_dir#*/archive/}"
        local backup_process=$(is_being_backed_up "${data_dir}")

        if [ "${backup_process}" ]; then
            message_info "${data_dir} is already being backed up by pid ${backup_process}"
            return
        fi
        backup_target_dir_full_path=${backup_target}/${visit}
    else
        backup_target_dir_full_path=${backup_target}
    fi

    message_empty
    message_info "${job_tag} Source: \"${data_dir}\""
    message_info "${job_tag} Target: \"${backup_target_dir_full_path}\""

    sudo -u ocs ssh -o "StrictHostKeyChecking accept-new" ${backup_user}@${backup_host} mkdir -p ${backup_target_dir_full_path#*:}

    if was_successfully_tranfered ${data_dir} > ${diffs}; then
        message_success "${job_tag} Was already transfered, skipping it"
        /bin/rm -f ${diffs}
        return
    fi


    local ndiffs=$(wc -l < ${diffs})

    #
    # If there are more than min_files_per_worker files per worker, split the workload
    #
    local min_files_per_worker=10
    local nworkers=${nprocs}
    local files_per_worker=$(( ( ndiffs / nworkers ) + 1 ))
    if (( files_per_worker <= min_files_per_worker )); then
        nworkers=1
        files_per_worker=${ndiffs}
    fi

    local working_dir=$(mktemp -d)
    local info_dir=${working_dir}/.info
    mkdir -p ${info_dir}

    pushd ${working_dir} >/dev/null 2>&1

    message_info "${job_tag} Splitting workload (${ndiffs} files) among ${nworkers} process(es) ..."

    split --lines=${files_per_worker} < ${diffs}
    cp ${diffs} ${info_dir}/files_list
    local chunks=( x?? )
    local failed_chunks=${info_dir}/failed_chunks
    /bin/rm -f ${diffs}

    local chunk_info_prefix
    for ((i = 0; i < ${#chunks[*]}; i++)); do
        (
            message_info "${job_tag} Synchronizing chunk #$((i+1)) (of ${#chunks[*]}) ..."
            local chunk_info_prefix="${info_dir}/chunk#$((i + 1))"
            local stat_file=${chunk_info_prefix}.status

            echo "sudo -u ocs rsync -aRz --stats --times ${rsync_extra_args} --files-from=${chunks[${i}]} ${data_dir}/ ${backup_target_dir_full_path} 2>/dev/null ) > ${stat_file}" > "${chunk_info_prefix}.cmd"
            cp ${chunks[${i}]} "${chunk_info_prefix}.files"
            (sudo -u ocs rsync -aRz -e 'ssh -T -c aes128-gcm@openssh.com -o Compression=no' --inplace --stats --times ${rsync_extra_args} --files-from=${chunks[${i}]} ${data_dir}/ ${backup_target_dir_full_path} 2>/dev/null ) > ${stat_file}
            local status=${?}
            echo "exit status: ${status}" >> ${stat_file}

            if [ "${status}" ] && [ ${status} -eq 0 ]; then
                Speedup=$(grep speedup ${stat_file}  | cut -d' ' -f8)
                    Bps=$(grep '^sent ' ${stat_file} | cut -d' ' -f9 | tr -d ,)
                   Mbps=$(awk -v Bps=${Bps} 'BEGIN { printf "%.3f\n", Bps / 125000 }')
                message_success "${job_tag} Chunk #$((i+1)) (of ${#chunks[*]}) succeeded (${Mbps} Mbps, speedup ${Speedup})"
            else
                # rsync returns 20 when killed
                if [ "${status}" != 20 ]; then
                    message_failure "${job_tag} Chunk #$((i+1)) (of ${#chunks[*]}) failed with status: ${status}"
                fi
                echo ${i} >> ${failed_chunks}
            fi
        ) &
    done

    wait

    if [ -e ${failed_chunks} ]; then
        message_failure "${job_tag} $(wc -l < ${failed_chunks}) chunks (out of ${#chunks[*]}) failed"
        /bin/rm -f ${failed_chunks}
    else
        # the backup succeeded
        stamp_as_transfered "${data_dir}" "${backup_target_dir_full_path}" "${job_tag}"
    fi

    popd >/dev/null 2>&1
    /bin/rm -rf ${working_dir}
}

#
# Main
#
if [ "${#candidates[@]}" -eq 0 ]; then     # no candidates were selected with --source
    if (( verbosity_level > 0 )); then message_info "Detecting candidates ..."; fi

    candidates=( $( find /$(hostname -s)/data[12]/archive -type d -path '*/proc/*' | grep -v '_re/' | sort ) )
fi

candidates_for_backup=()
candidates_for_removal=()
missing_ready_stamp=()
missing_status_file=()
still_young=()
candidates_for_retry=()
declare -A hours_from_ready=()

#
# The preamble.
# Gather information about the candidates (either specified by the user or found on the filesystems)
#
for candidate in ${candidates[*]}; do
    stat_file=${candidate}/.status
    time_stamp=
    hours=

    if [ ! -r ${stat_file} ]; then
        missing_status_file+=( ${candidate} )
        continue
    fi

    time_stamp=$(grep -w "ready-for-transfer" ${stat_file} | tail -1 | cut -d' ' -f1)
    if [ ! "${time_stamp}" ]; then
        missing_ready_stamp+=( ${candidate} )
        continue
    fi

    hours_from_ready[${candidate}]=${hours}
    hours=$(( ( $(date +%s) - $(date --date "${time_stamp}" +%s) ) / 3600 ))

    if grep -qw transfered ${stat_file}; then

        if (( hours >= old_age_hours )); then
            candidates_for_removal+=( ${candidate} )
        else
            still_young+=( ${candidate} )
        fi

    else
        candidates_for_backup+=( ${candidate} )
    fi

done

#
# The status part
#
if ${status_flag}; then
    message_info "Found ${#candidates[*]} candidates"

    message_empty
    message_info "Found $(printf "%4d" ${#missing_status_file[*]}) candidates with no .status file"
    if (( verbosity_level > 0 )); then
        for candidate in ${missing_status_file[*]}; do
            message_info "  ${candidate}"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#missing_ready_stamp[*]}) candidates with no ready-for-transfer stamp"
    if (( verbosity_level > 0 )); then
        for candidate in ${missing_ready_stamp[*]}; do
            message_info "  ${candidate}"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#candidates_for_backup[*]}) candidates for backup"
    if (( verbosity_level > 0 )); then
        for candidate in ${candidates_for_backup[*]}; do
            message_info "  ${candidate}"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#candidates_for_removal[*]}) candidates old enough for removal (ready-for-transfer for more than ${old_age_hours} hours)"
    if (( verbosity_level > 0 )); then
        for candidate in ${candidates_for_removal[*]}; do
            message_info "  ${candidate}: $(printf "%4d" ${hours_from_ready[${candidate}]}) hours"
        done
    fi

    if (( verbosity_level > 0 )); then message_empty; fi
    message_info "Found $(printf "%4d" ${#still_young[*]}) candidates too young for removal (ready-for-transfer for less than ${old_age_hours} hours)"
    if (( verbosity_level > 0 )); then
        for candidate in ${still_young[*]}; do
            message_info "  ${candidate}: $(printf "%4d" ${hours_from_ready[${candidate}]}) hours"
        done
    fi

    if ${running_flag}; then
        show_running_processes
    fi

    exit 0
fi

if ${running_flag} && ! ${status_flag}; then
    show_running_processes
    exit 0
fi


#
# The removal part
# Remove local directories only if they are the same as the backup
#
if ${remove_flag} && [ ${#candidates_for_removal[*]} -ne 0 ]; then
    message_info "Considering ${#candidates_for_removal[*]} candidates for removal ..."

    for candidate in ${candidates_for_removal[*]}; do
        diffs=$(mktemp)
        if was_successfully_tranfered ${candidate} >${diffs}; then
            /bin/rm -rf ${candidate}
            message_success "  ${candidate} - removed"
        else
            message_error "  ${candidate} - differs from backup ($(wc -l < ${diffs}) diffs), will retry"
            candidates_for_retry+=( ${candidate} )  # give it another chance
        fi
        /bin/rm ${diffs}
    done
fi

#
# The backup part
# Backup candidates that are ready-for-transfer but:
#  - are not marked-as-transfered 
#  - are marked-as-transfered but not really backed up (either backup error or different from the backup)
#

total_candidates_for_backup=( ${candidates_for_backup[*]} ${candidates_for_retry[*]} )
if [ ${#total_candidates_for_backup[*]} -ne 0 ]; then
    if (( verbosity_level > 0 )); then
        message_info "Considering ${#total_candidates_for_backup[*]} candidates for backup (or retry)"
    fi

    running_jobs=0
    job_id=0
    for candidate in ${total_candidates_for_backup[*]}; do
        (( job_id++ ))

        backup_process=$(is_being_backed_up ${candidate})
        if [ "${backup_process}" ]; then
            message_success "${candidate}: is currently being backed up by process ${backup_process}, skipped"
            continue
        fi

        backup_one_candidate ${candidate} "[${job_id}/${#total_candidates_for_backup[*]}]" &
        (( running_jobs++ ))

        if ((running_jobs >= max_jobs)); then
            wait -n
            ((running_jobs--))
        fi
    done
else
    if (( verbosity_level > 0 )); then
        message_ok "No candidates for backup"
    fi
fi

cleanup
